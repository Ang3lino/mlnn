{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 2. Neural Networks \n",
    "\n",
    "## Mathematical background\n",
    "\n",
    "\n",
    "\n",
    "In this notebook we review the concepts of KL divergence, Likelihood, and cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "#import tensorflow as tf\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback Leibler divergence\n",
    "\n",
    "The Kullback Leibler is a divergence measure that quatifies the difference between two distributions. \n",
    "\n",
    "For continuous random variable it is computed as an integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$KL(p,q) = \\int p(x)*log \\left(\\frac{p(x)}{q(x)} \\right) dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For discrete random variables it is computed as a summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$KL(p,q) = \\sum_{i=1}^n p(x^i)*log \\left(\\frac{p(x^i)}{q(x^i)} \\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts an integer to binary of 8 positions\n",
    "def  Binary_8(x):\n",
    "    binary = np.array([x], dtype=np.uint8)\n",
    "    return binary\n",
    "    \n",
    "# Creates a joint distribution for 8 binary variables based on function f\n",
    "def  Joint_Distribution(f):\n",
    "    n = 8\n",
    "    dist = [ f(Binary_8(x)) for x in range(2 ** n)]\n",
    "    return np.array(dist/np.sum(dist))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\n",
      "[[ 0.        ]\n",
      " [ 0.00350877]\n",
      " [ 0.01403509]\n",
      " [ 0.03157895]\n",
      " [ 0.05614035]\n",
      " [ 0.0877193 ]\n",
      " [ 0.12631579]\n",
      " [ 0.17192982]\n",
      " [ 0.2245614 ]\n",
      " [ 0.28421053]]\n"
     ]
    }
   ],
   "source": [
    "print(Binary_8(8))\n",
    "print(Joint_Distribution(lambda x: x ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Complete the implementation of the Kullback Leibler function below and evaluate quantifying the divergence\n",
    "between the three distributions in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p: p(x1), p(x2), ...,\n",
    "# q\n",
    "\n",
    "def Kullback_Leibler(p,q):\n",
    "    KL = 0\n",
    "    for pi, qi in zip(p, q):\n",
    "        if pi == 0 or qi == 0: \n",
    "            continue\n",
    "        KL += pi * np.log((pi / qi))\n",
    "    return KL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The KL divergence between the distributions are:  [ 0.08900492] 3.49533672409 [ 3.72419103]\n"
     ]
    }
   ],
   "source": [
    "# Distribution associated to function f1 \n",
    "f1 = np.sum\n",
    "dist1 = Joint_Distribution(f1)\n",
    "\n",
    "# Distribution associated to function f2\n",
    "f2 = np.square\n",
    "dist2 = Joint_Distribution(f2)\n",
    "\n",
    "# Uniform distribution\n",
    "dist3 = np.full(2 ** 8, 1 / 256)\n",
    "\n",
    "kl_12 = Kullback_Leibler(dist1,dist2)\n",
    "kl_13 = Kullback_Leibler(dist1,dist3)\n",
    "kl_23 = Kullback_Leibler(dist2,dist3)\n",
    "\n",
    "print(\"The KL divergence between the distributions are: \",kl_12,kl_13,kl_23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    " The function scipy.stats.entropy allows the computation of the KL (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html ). Compare the results of this function with Kullback_Leibler for computing the divergences kl_21 and kl_31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl_21:  0.0\n",
      "kl_21:  0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "kl_21 = Kullback_Leibler(dist2, dist1)\n",
    "kl_31 = Kullback_Leibler(dist3, dist1)\n",
    "\n",
    "print(\"kl_21: \", entropy(kl_21))\n",
    "print(\"kl_31: \", entropy(kl_31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood and Log-likelihood\n",
    "\n",
    "\n",
    "In the lecture we have seen the the maximum likelihood estimator is the most used way to estimate the parameters of a model. For a parametric distribution of parameter $\\theta$, the loglikelihood is computed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LL = $\\sum_{i=1}^n   p_{\\theta}(x^i)log(p(x^i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and usually we want to find the parameter $\\theta$ that maximizes the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Let $\\theta = (\\hat{\\mu},\\hat{\\sigma})$ be the parameters of a Gaussian distribution. Given a list of samples S  and $\\theta$ as inputs, complete in the next cell the implementation of the function Log_Likelihood() that computes the log_likelihood for the sample S given the model and parameters theta.  \n",
    "\n",
    "Suggestion: Use function vals = norm.pdf(x,mu,sigma) that computes the Gaussian probability assigned to point x\n",
    "by a Gaussian distribution of parameters mu,sigma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0322408887722\n",
      "0.977900178479\n"
     ]
    }
   ],
   "source": [
    "def Log_Likelihood(S,theta):\n",
    "    mu, sigma = theta[0], theta[1]    \n",
    "    LL = 0 \n",
    "    for x in S:\n",
    "        p = norm.pdf(x, mu, sigma)\n",
    "        LL += p * math.log2(p)\n",
    "    return LL\n",
    "    \n",
    "    \n",
    "param_theta = [20,24]    \n",
    "set_S = [79.19707822, 107.80002586, 112.87202068, 110.24491734, 84.36827062, 112.3651777 , 104.68415497, 135.02754932,\n",
    "     104.7278481 ,  90.23410368, 121.39874476,  74.19135635, 102.09448052,  97.62711719, 151.56860878]\n",
    "\n",
    "ll = Log_Likelihood(set_S, param_theta)\n",
    "print(ll)\n",
    "print(2 ** ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "\n",
    " Given a list of possible theta parameters (all_thetas), determine which is the theta that better fits the samples in set_S. \n",
    " \n",
    " \n",
    " Suggestion: Use function  Log_Likelihood() implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.782684623687\n"
     ]
    }
   ],
   "source": [
    "set_S = [ 79.19707822, 107.80002586, 112.87202068, 110.24491734, 84.36827062, 112.3651777 , 104.68415497, 135.02754932,\n",
    "     104.7278481 ,  90.23410368, 121.39874476,  74.19135635, 102.09448052,  97.62711719, 151.56860878]\n",
    "\n",
    "all_thetas = [[125,20],[100,30],[90,11],[135,24],[110,30],[108,11]]\n",
    "\n",
    "\n",
    "max_index = np.argmax([Log_Likelihood(set_S, theta) for theta in all_thetas])\n",
    "print(Log_Likelihood(set_S, all_thetas[max_index]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  Kullback Leibler, entropy, and cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go back to the definition of the Kullback Leibler difference: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "   KL(p,q) &= \\sum_{i=1}^n p(x^i)*log \\left(\\frac{p(x^i)}{q(x^i)} \\right) \\\\\n",
    "           &= \\sum_{i=1}^n p(x^i)*log \\;p(x^i)  -\\sum_{i=1}^n p(x^i)* log \\;q(x^i)\\\\\n",
    "           &=  H(p) + H(p,q) \\\\\n",
    "\\end{align} \n",
    "\n",
    "where $H(p)$ is the entropy of $p$ and $H(p,q)=-\\sum_{i=1}^n p(x^i)* log \\;q(x^i)$ is called the crossentropy between distributions $p$ and $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see then that the Kullback Leibler divergence between two distributions is the sum of the entropy of $p$ plus the cross entropy between p and q. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-likelihood and Cross-entropy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, there is a strong relationship between the log-likelihood and the cross-entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "   LL      &=  \\sum_{i=1}^n   p_{\\theta}(x^i)log(p(x^i)) \\\\\n",
    "           &=  -H(p_{\\theta},p)\n",
    "\\end{align}    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, maximizing the log-likelihood is equivalent to minimizing the cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise we use the implementation of a logistic-regression classifier from the previous Lab. This implementation is an adaptation of that found in the *Python machine learning* book, Raschka, S., & Mirjalili, V. (2017). Packt Publishing Ltd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression(object):\n",
    "    def __init__(self, eta=0.01, n_iter=1000, random_state=0):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w) + self.b\n",
    "    \n",
    "    def activation(self, z):\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -25, 25)))\n",
    "        \n",
    "    def fit(self, X, y):        \n",
    "        self.w = self.rgen.normal(loc=0, scale=0.01, size=X.shape[1])\n",
    "        self.b = self.rgen.normal(loc=0, scale=0.01, size=1)\n",
    "        self.cost = []\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = y-output                                # The error is computed as the difference between the \n",
    "                                                            # prob. of the class and the prediction of the model\n",
    "            \n",
    "            self.w += self.eta * X.T.dot(errors)\n",
    "            self.b = self.eta * errors.sum()\n",
    "            \n",
    "            cost = (-y.dot(np.log(output)) - ((1-y).dot(np.log(1-output))))\n",
    "            #print(i,cost,errors.sum())\n",
    "            self.cost.append(cost)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predicted_class = self.predict_proba(X) > 0.5\n",
    "        return predicted_class  # Given the features \"predict\" outputs the classification given by the model\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        predicted_proba = self.activation(self.net_input(X))\n",
    "        return predicted_proba  # Given the features predict_proba outputs the probability that the solution belongs to the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "\n",
    "  a) Identify in the function \"fit\" where the cross-entropy is computed.\n",
    "  \n",
    "  b) Visualize the value of the cross-entropy for the different iterations.\n",
    "  \n",
    "  c) Evaluate what happens when difficult data is used (Uncomment function tr_data,c = Create_Difficult_Classification_Data(npoints,npoints))\n",
    "  \n",
    "  d) Could you interpret what the curve of the cross-entropy tells us about the performance of the algorithm?\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function from previous Lab used to create some classification data\n",
    "\n",
    "def Create_Classification_Data(number_points_Class_A,number_points_Class_B):    \n",
    "    \n",
    "    # Points in Class A\n",
    "    xA = 20*np.random.rand(number_points_Class_A)\n",
    "    shiftA = 20*np.random.rand(number_points_Class_A)\n",
    "    yA = (4+xA)/2.0 - shiftA - 0.1\n",
    "\n",
    "    # Points in Class B\n",
    "    xB = 20*np.random.rand(number_points_Class_B)\n",
    "    shiftB = 20*np.random.rand(number_points_Class_B)\n",
    "    yB = (4+xB)/2.0 + shiftB + 0.1\n",
    "\n",
    "    \n",
    "    c = np.hstack((np.ones((number_points_Class_A)),np.zeros((number_points_Class_B))))\n",
    "    #print(c.shape)\n",
    "\n",
    "    # We create the training data concatenating examples from the two classes XA and XB\n",
    "    tr_data = np.hstack((np.vstack((xA,yA)),np.vstack((xB,yB)))).transpose()\n",
    "    #print(training_data.shape)\n",
    "\n",
    "    return tr_data,c\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function from previous Lab used to create some classification data\n",
    "\n",
    "def Create_Difficult_Classification_Data(number_points_Class_A,number_points_Class_B):    \n",
    "    \n",
    "    # Points in Class A\n",
    "    xA1 = 20*np.random.rand(number_points_Class_A)\n",
    "    shiftA1 = 20*np.random.rand(number_points_Class_A)\n",
    "    yA1 = (4+xA1)/2.0 - shiftA1 + 5.0\n",
    "\n",
    "    # Points in Class B\n",
    "    xB1 = 20*np.random.rand(number_points_Class_B)\n",
    "    shiftB1 = 20*np.random.rand(number_points_Class_B)\n",
    "    yB1 = (4+xB1)/2.0 + shiftB1 - 5.0\n",
    "\n",
    "    # Sinusoidal curve dividing the two classes      \n",
    "    x2 = np.linspace(0, 20, 2000)\n",
    "    y2 = 20*np.cos(0.2*np.pi*x2) \n",
    "    \n",
    "    c = np.hstack((np.ones((number_points_Class_A)),np.zeros((number_points_Class_B))))\n",
    "    #print(c.shape)\n",
    "\n",
    "    # We create the training data concatenating examples from the two classes XA and XB\n",
    "    tr_data = np.hstack((np.vstack((xA1,yA1)),np.vstack((xB1,yB1)))).transpose()\n",
    "    #print(training_data.shape)\n",
    "\n",
    "    return tr_data,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points in each class\n",
    "npoints = 150\n",
    "# We generate the data for classification\n",
    "tr_data,c = Create_Classification_Data(npoints,npoints)\n",
    "#tr_data,c = Create_Difficult_Classification_Data(npoints,npoints)\n",
    "\n",
    "\n",
    "# We define the LogisticRegression object\n",
    "mylr = MyLogisticRegression(eta=0.01, n_iter=20, random_state=10)\n",
    "\n",
    "# Logistic regression learns from data\n",
    "mylr = mylr.fit(tr_data,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function to draw the cross-entropy\n",
    "def plot_cross_entropy(points,fsize):\n",
    "    plt.figure()\n",
    "    plt.xlabel(r'$x$', fontsize=fsize)\n",
    "    plt.ylabel(r'$y$', fontsize=fsize)\n",
    "    plt.plot(points,'-m',lw=4)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_entropy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f925fff484c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_entropy' is not defined"
     ]
    }
   ],
   "source": [
    "# plot_cross_entropy(cross_entropy,14)\n",
    "\n",
    "plot_cross_entropy(cross_entropy, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
