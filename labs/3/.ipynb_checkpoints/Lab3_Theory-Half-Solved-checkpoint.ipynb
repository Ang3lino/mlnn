{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 2. Neural Networks \n",
    "\n",
    "## Mathematical background\n",
    "\n",
    "\n",
    "\n",
    "In this notebook we review the concepts of KL divergence, Likelihood, and cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "#import tensorflow as tf\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback Leibler divergence\n",
    "\n",
    "The Kullback Leibler is a divergence measure that quatifies the difference between two distributions. \n",
    "\n",
    "For continuous random variable it is computed as an integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$KL(p,q) = \\int p(x)*log \\left(\\frac{p(x)}{q(x)} \\right) dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For discrete random variables it is computed as a summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$KL(p,q) = \\sum_{i=1}^n p(x^i)*log \\left(\\frac{p(x^i)}{q(x^i)} \\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts an integer to binary of 8 positions\n",
    "def  Binary_8(x):\n",
    "    binary = np.array([x], dtype=np.uint8)\n",
    "    return binary\n",
    "    \n",
    "# Creates a joint distribution for 8 binary variables based on function f\n",
    "def  Joint_Distribution(f):\n",
    "    n = 8\n",
    "    dist = [ f(Binary_8(x)) for x in range(2^n)]\n",
    "    return np.array(dist/np.sum(dist))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Complete the implementation of the Kullback Leibler function below and evaluate quantifying the divergence\n",
    "between the three distributions in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kullback_Leibler(p,q):\n",
    "    KL = 0\n",
    "    \n",
    "    return KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The KL divergence between the distributions are:  0 0 0\n"
     ]
    }
   ],
   "source": [
    "# Distribution associated to function f1 \n",
    "f1 = np.sum\n",
    "dist1 = Joint_Distribution(f1)\n",
    "\n",
    "# Distribution associated to function f2\n",
    "f2 = np.square\n",
    "dist2 = Joint_Distribution(f2)\n",
    "\n",
    "# Uniform distribution\n",
    "dist3 = np.full(8,0.1)\n",
    "\n",
    "kl_12 = Kullback_Leibler(dist1,dist2)\n",
    "kl_13 = Kullback_Leibler(dist1,dist3)\n",
    "kl_23 = Kullback_Leibler(dist2,dist3)\n",
    "\n",
    "print(\"The KL divergence between the distributions are: \",kl_12,kl_13,kl_23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    " The function scipy.stats.entropy allows the computation of the KL (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html ). Compare the results of this function with Kullback_Leibler for computing the divergences kl_21 and kl_31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood and Log-likelihood\n",
    "\n",
    "\n",
    "In the lecture we have seen the the maximum likelihood estimator is the most used way to estimate the parameters of a model. For a parametric distribution of parameter $\\theta$, the loglikelihood is computed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LL = $\\sum_{i=1}^n   p_{\\theta}(x^i)log(p(x^i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and usually we want to find the parameter $\\theta$ that maximizes the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Let $\\theta = (\\hat{\\mu},\\hat{\\sigma})$ be the parameters of a Gaussian distribution. Given a list of samples S  and $\\theta$ as inputs, complete in the next cell the implementation of the function Log_Likelihood() that computes the log_likelihood for the sample S given the model and parameters theta.  \n",
    "\n",
    "Suggestion: Use function vals = norm.pdf(x,mu,sigma) that computes the Gaussian probability assigned to point x\n",
    "by a Gaussian distribution of parameters mu,sigma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XXX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b849bae051b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m      104.7278481 ,  90.23410368, 121.39874476,  74.19135635, 102.09448052,  97.62711719, 151.56860878]\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLog_Likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_theta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b849bae051b2>\u001b[0m in \u001b[0;36mLog_Likelihood\u001b[0;34m(S, theta)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mLL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mXXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mLL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XXX' is not defined"
     ]
    }
   ],
   "source": [
    "def Log_Likelihood(S,theta):\n",
    "    mu = theta[0]\n",
    "    sigma = theta[1]    \n",
    "    LL = 0 \n",
    "    for x in S:\n",
    "        XXX\n",
    "    return LL\n",
    "    \n",
    "    \n",
    "param_theta = [20,24]    \n",
    "set_S = [ 79.19707822, 107.80002586, 112.87202068, 110.24491734, 84.36827062, 112.3651777 , 104.68415497, 135.02754932,\n",
    "     104.7278481 ,  90.23410368, 121.39874476,  74.19135635, 102.09448052,  97.62711719, 151.56860878]\n",
    "\n",
    "ll = Log_Likelihood(set_S,param_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "\n",
    " Given a list of possible theta parameters (all_thetas), determine which is the theta that better fits the samples in set_S. \n",
    " \n",
    " \n",
    " Suggestion: Use function  Log_Likelihood() implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_S = [ 79.19707822, 107.80002586, 112.87202068, 110.24491734, 84.36827062, 112.3651777 , 104.68415497, 135.02754932,\n",
    "     104.7278481 ,  90.23410368, 121.39874476,  74.19135635, 102.09448052,  97.62711719, 151.56860878]\n",
    "\n",
    "all_thetas = [[125,20],[100,30],[90,11],[135,24],[110,30],[108,11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  Kullback Leibler, entropy, and cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go back to the definition of the Kullback Leibler difference: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "   KL(p,q) &= \\sum_{i=1}^n p(x^i)*log \\left(\\frac{p(x^i)}{q(x^i)} \\right) \\\\\n",
    "           &= \\sum_{i=1}^n p(x^i)*log \\;p(x^i)  -\\sum_{i=1}^n p(x^i)* log \\;q(x^i)\\\\\n",
    "           &=  H(p) + H(p,q) \\\\\n",
    "\\end{align} \n",
    "\n",
    "where $H(p)$ is the entropy of $p$ and $H(p,q)=-\\sum_{i=1}^n p(x^i)* log \\;q(x^i)$ is called the crossentropy between distributions $p$ and $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see then that the Kullback Leibler divergence between two distributions is the sum of the entropy of $p$ plus the cross entropy between p and q. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-likelihood and Cross-entropy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, there is a strong relationship between the log-likelihood and the cross-entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "   LL      &=  \\sum_{i=1}^n   p_{\\theta}(x^i)log(p(x^i)) \\\\\n",
    "           &=  -H(p_{\\theta},p)\n",
    "\\end{align}    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, maximizing the log-likelihood is equivalent to minimizing the cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise we use the implementation of a logistic-regression classifier from the previous Lab. This implementation is an adaptation of that found in the *Python machine learning* book, Raschka, S., & Mirjalili, V. (2017). Packt Publishing Ltd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression(object):\n",
    "    def __init__(self, eta=0.01, n_iter=1000, random_state=0):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w) + self.b\n",
    "    \n",
    "    def activation(self, z):\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -25, 25)))\n",
    "        \n",
    "    def fit(self, X, y):        \n",
    "        self.w = self.rgen.normal(loc=0, scale=0.01, size=X.shape[1])\n",
    "        self.b = self.rgen.normal(loc=0, scale=0.01, size=1)\n",
    "        self.cost = []\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = y-output                                # The error is computed as the difference between the \n",
    "                                                            # prob. of the class and the prediction of the model\n",
    "            \n",
    "            self.w += self.eta * X.T.dot(errors)\n",
    "            self.b = self.eta * errors.sum()\n",
    "            \n",
    "            cost = (-y.dot(np.log(output)) - ((1-y).dot(np.log(1-output))))\n",
    "            #print(i,cost,errors.sum())\n",
    "            self.cost.append(cost)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predicted_class = self.predict_proba(X)>0.5\n",
    "        return predicted_class # Given the features \"predict\" outputs the classification given by the model\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        predicted_proba = activation(net_input(X))\n",
    "        return predicted_proba # Given the features predict_proba outputs the probability that the solution belongs to the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "\n",
    "  a) Identify in the function \"fit\" where the cross-entropy is computed.\n",
    "  \n",
    "  b) Visualize the value of the cross-entropy for the different iterations.\n",
    "  \n",
    "  c) Evaluate what happens when difficult data is used (Uncomment function tr_data,c = Create_Difficult_Classification_Data(npoints,npoints))\n",
    "  \n",
    "  d) Could you interpret what the curve of the cross-entropy tells us about the performance of the algorithm?\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function from previous Lab used to create some classification data\n",
    "\n",
    "def Create_Classification_Data(number_points_Class_A,number_points_Class_B):    \n",
    "    \n",
    "    # Points in Class A\n",
    "    xA = 20*np.random.rand(number_points_Class_A)\n",
    "    shiftA = 20*np.random.rand(number_points_Class_A)\n",
    "    yA = (4+xA)/2.0 - shiftA - 0.1\n",
    "\n",
    "    # Points in Class B\n",
    "    xB = 20*np.random.rand(number_points_Class_B)\n",
    "    shiftB = 20*np.random.rand(number_points_Class_B)\n",
    "    yB = (4+xB)/2.0 + shiftB + 0.1\n",
    "\n",
    "    \n",
    "    c = np.hstack((np.ones((number_points_Class_A)),np.zeros((number_points_Class_B))))\n",
    "    #print(c.shape)\n",
    "\n",
    "    # We create the training data concatenating examples from the two classes XA and XB\n",
    "    tr_data = np.hstack((np.vstack((xA,yA)),np.vstack((xB,yB)))).transpose()\n",
    "    #print(training_data.shape)\n",
    "\n",
    "    return tr_data,c\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function from previous Lab used to create some classification data\n",
    "\n",
    "def Create_Difficult_Classification_Data(number_points_Class_A,number_points_Class_B):    \n",
    "    \n",
    "    # Points in Class A\n",
    "    xA1 = 20*np.random.rand(number_points_Class_A)\n",
    "    shiftA1 = 20*np.random.rand(number_points_Class_A)\n",
    "    yA1 = (4+xA1)/2.0 - shiftA1 + 5.0\n",
    "\n",
    "    # Points in Class B\n",
    "    xB1 = 20*np.random.rand(number_points_Class_B)\n",
    "    shiftB1 = 20*np.random.rand(number_points_Class_B)\n",
    "    yB1 = (4+xB1)/2.0 + shiftB1 - 5.0\n",
    "\n",
    "    # Sinusoidal curve dividing the two classes      \n",
    "    x2 = np.linspace(0, 20, 2000)\n",
    "    y2 = 20*np.cos(0.2*np.pi*x2) \n",
    "    \n",
    "    c = np.hstack((np.ones((number_points_Class_A)),np.zeros((number_points_Class_B))))\n",
    "    #print(c.shape)\n",
    "\n",
    "    # We create the training data concatenating examples from the two classes XA and XB\n",
    "    tr_data = np.hstack((np.vstack((xA1,yA1)),np.vstack((xB1,yB1)))).transpose()\n",
    "    #print(training_data.shape)\n",
    "\n",
    "    return tr_data,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points in each class\n",
    "npoints = 150\n",
    "# We generate the data for classification\n",
    "tr_data,c = Create_Classification_Data(npoints,npoints)\n",
    "#tr_data,c = Create_Difficult_Classification_Data(npoints,npoints)\n",
    "\n",
    "\n",
    "# We define the LogisticRegression object\n",
    "mylr = MyLogisticRegression(eta=0.01, n_iter=20, random_state=10)\n",
    "\n",
    "# Logistic regression learns from data\n",
    "mylr = mylr.fit(tr_data,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function to draw the cross-entropy\n",
    "def plot_cross_entropy(points,fsize):\n",
    "    plt.figure()\n",
    "    plt.xlabel(r'$x$', fontsize=fsize)\n",
    "    plt.ylabel(r'$y$', fontsize=fsize)\n",
    "    plt.plot(points,'-m',lw=4)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_entropy(cross_entropy,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
