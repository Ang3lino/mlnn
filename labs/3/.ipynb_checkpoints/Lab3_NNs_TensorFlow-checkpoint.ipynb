{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 2. Neural Networks \n",
    "## Mathematical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: The examples and exercises used in this lab are use element from exercises and examples presented in:\n",
    "        https://github.com/hamarex/tensorflow/blob/master/jupyter_tfbook/Chapter02/Maximum%20likelihood%20estimation%20example.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all the libraries required for the exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Parkinsons Telemonitoring Data Set available from https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/\n",
    "    \n",
    "    This dataset contains 16 biomedical voice measurements from 42 people with early-stage Parkinson's disease. \n",
    "    \n",
    "    The main aim of the data is to predict the motor and total UPDRS scores ('motor_UPDRS' and 'total_UPDRS') from the 16 voice measures. \n",
    "\n",
    "    This can be seen as a regression problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset  and open using the following commands\n",
    "\n",
    "# Stores the data as a dataframe of Pandas\n",
    "df = pd.read_csv('parkinsons_updrs.data')\n",
    "print(df.columns)\n",
    "\n",
    "# There are 42 subjects. We will use data for the first one\n",
    "df_subject_1 = df[df['subject#']==1]\n",
    "data = df_subject_1.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualize the relationships between variables in the dataset using seaborn\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = df.std().sort_values().index\n",
    "sns.lvplot(data=df, order=order, scale=\"linear\")\n",
    "plt.show()\n",
    "# https://seaborn.pydata.org/examples/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sca = df - df.min()\n",
    "df_sca /= df_sca.max()\n",
    "sns.lvplot(data=df_sca, order=order, scale=\"linear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the data, we select the response variable that we are going to model\n",
    "# It is the total_UPDRS score\n",
    "target = data[:,5].reshape([len(data), 1])\n",
    "\n",
    "# The 16 variables that measure the voice will be used as features. \n",
    "features = data[:,6:22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the dataset for the first subject in training and test data. Even rows are in the train set and odd rows in the test set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train set \n",
    "train_features = features[::2,:]\n",
    "train_target = target[::2]\n",
    "train_n_samples = train_features.shape[0]\n",
    "\n",
    "# Test set\n",
    "test_features = features[1::2,:]\n",
    "test_target = target[1::2]\n",
    "test_n_samples = test_features.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first analysis we will use only one feature to model the response variable\n",
    "We select the \"best predictor\" among the 16 voice features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SelectKBest function from scikit-learn, with the f_regression metric, is used for feature selection\n",
    "feature_selector = SelectKBest(f_regression, k=1)\n",
    "feature_selector.fit(train_features, train_target)\n",
    "\n",
    "# We print which is the selected feature\n",
    "print(feature_selector.get_support(), \",\", df.columns[6:22][feature_selector.get_support()][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the feature and the response variable to inspect the relationship between the two variables visually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sel_feature = feature_selector.transform(train_features)\n",
    "test_sel_feature = feature_selector.transform(test_features)\n",
    "plt.plot(train_sel_feature,train_target,'.')\n",
    "plt.xlabel(df.columns[6:22][feature_selector.get_support()][0])\n",
    "plt.ylabel('Response variable ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Complete the missing parameters in the following line to obtain the prediction of the response variable in the test data from a model learned in the train data.\n",
    "\n",
    "Check the quality of the prediction by plotting real versus predicted values. This can be done by running the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_target = (LinearRegression().fit()).predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_sel_feature, test_target, 'b.', label='Original data')\n",
    "plt.plot(test_sel_feature,predicted_test_target,'vr', label='Scikit-learn LR') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tensorflow to learn a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use tensorflow to learn a linear regression model similar to the one learned using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# We set the constant operators that will store the values of the \n",
    "# feature and response variable\n",
    "\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weights and bias are defined as variables. Notice that the definition is\n",
    "# different to the one used above for the features and the target variable\n",
    "# W and b will be initialized to zero.  By default, Variables are trainable.\n",
    "\n",
    "W = tf.Variable(0.0, name=\"weight\")\n",
    "b = tf.Variable(0.0, name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the linear model y = wx+b\n",
    "# Notice that there is only one reponse variable and only one weight \n",
    "\n",
    "predicted = tf.add(tf.multiply(X, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function to use will be the mean squared error\n",
    "loss = tf.reduce_sum(tf.pow(predicted-Y, 2))/(train_n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the parameters of the learning process \n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimizer is the gradient descent\n",
    "# It uses as input parameter the learning rate and the loss function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting the tensorflow session we need to initialize the parameters\n",
    "# We init tensorflow variables\n",
    "\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Number of iterations for the learning algorithm\n",
    "training_epochs = 1000\n",
    "display_step = 50\n",
    "perm = np.random.permutation(train_n_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three classes of gradient descent methods\n",
    "\n",
    "- Batch gradient descent\n",
    "\n",
    "- Stochastic gradient descent \n",
    "\n",
    "- Mini-batch gradient descent \n",
    "\n",
    "\n",
    "Below there is an example of Mini-batch gradient descent implemented with the classical gradient decent minimizer method.\n",
    "\n",
    "After the model has been learned we plot the predictions in the test set. Uncomment the line where the prediction for the scikit-learn model is plotted in order to compare the prediction of the two algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 6\n",
    "n_batch = train_n_samples // mini_batch_size + (train_n_samples % mini_batch_size != 0)\n",
    "error = []\n",
    "\n",
    "# Start training\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit the training data in the batches.\n",
    "    for epoch in range(training_epochs):\n",
    "        i_batch = (epoch % n_batch)*mini_batch_size\n",
    "        batch = train_sel_feature[i_batch:i_batch+mini_batch_size], train_target[i_batch:i_batch+mini_batch_size]\n",
    "        sess.run(optimizer, feed_dict={X: batch[0], Y: batch[1]})\n",
    "        \n",
    "        #for (x, y) in zip(train_sel_feature[perm,:], train_target[perm]):\n",
    "        #    sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(loss, feed_dict={X: train_sel_feature, Y: train_target})\n",
    "            error.append(c)        \n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c),  \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "                  \n",
    "       \n",
    "   \n",
    "    training_loss = sess.run(loss, feed_dict={X: train_sel_feature , Y: train_target})\n",
    "    print(\"Training loss=\", training_loss, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
    "   \n",
    "    \n",
    "    # Visualization of the prediction for the test data\n",
    "    plt.plot(test_sel_feature, test_target, 'b.', label='Original data')\n",
    "    plt.plot(test_sel_feature, sess.run(W) * test_sel_feature + sess.run(b),'om', label='tensorflow LR')\n",
    "    #plt.plot(test_sel_feature,predicted_test_target,'vr', label='Scikit-learn LR')\n",
    "    plt.xlabel('Voice feature')\n",
    "    plt.ylabel('Response variable ')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 \n",
    "\n",
    "a) Use tensorflow to learn the model y = a*x^2 + c*x + b where are a,b, and c are the parameters of a linear model.\n",
    "\n",
    "b) Train the model using two different stochastic gradient descent strategies and the Momentum optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "a) Use tensorflow to learn the model y = ax + c*z + b where are a,b, and c are the parameters of a linear model and x and z are the two most \n",
    "informative features among the 16 variables. \n",
    "\n",
    "\n",
    "b) Train the model using the batch gradient decent and the Adam classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the parameters of a Logistic Regression Classifier with tensorflow\n",
    "\n",
    "We will consider now a classification problem with 16 features\n",
    "\n",
    "\n",
    "For this problem we will learn the parameters of a logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a first step we create the class variable (label)\n",
    "# binarizing the response variable we had used for regression\n",
    "scaler = StandardScaler()\n",
    "label = binarize(scaler.fit(target).transform(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class variable is split into train and test sets\n",
    "train_label = label[::2]\n",
    "test_label = label[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the variables and constants of the tensorflow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# The input will be a matrix with 16 columns (one for each feature)\n",
    "x = tf.placeholder(tf.float32, [None, 16])\n",
    "\n",
    "# Class binary variable\n",
    "t = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# There will be 16 weights, one corresponding to each feature \n",
    "w = tf.Variable(tf.zeros([16, 1]))\n",
    "\n",
    "# This is the intersect, usually also denoted as b\n",
    "b = tf.Variable(tf.zeros([1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the elements of the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model, matrix multiplication of the variables and the weight\n",
    "linear_model = tf.matmul(x, w) + b\n",
    "\n",
    "# The probability is computed combining the linear model and the sigmoid function\n",
    "LR_prob = tf.sigmoid(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined the loss function and the accuracy measures used to evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function will be maximum likelihood estimation for the logistic regression model\n",
    "\n",
    "loss = -tf.reduce_sum(t*tf.log(LR_prob) + (1-t)*tf.log(1-LR_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will find the parameters of the model (the weights)\n",
    "# by minimizing the loglikelihood with the AdapOptimizer\n",
    "\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether, for a given instance, the prediction is correct\n",
    "correct_prediction = tf.equal(tf.sign(LR_prob-0.5), tf.sign(LR_prob-0.5))\n",
    "\n",
    "# The accuracy, number of correct predictions divided by the number of examples\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "#sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "\n",
    "# Initialization of the global variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auxiliary data for training the model\n",
    "\n",
    "train_x = train_features\n",
    "train_t = train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "sess.run(init)\n",
    "i = 0\n",
    "for _ in range(20000):\n",
    "    i += 1\n",
    "    sess.run(train_step, feed_dict={x:train_x, t:train_t})\n",
    "    if i % 500 == 0:\n",
    "        loss_val, acc_val = sess.run(\n",
    "            [loss, accuracy], feed_dict={x:train_x, t:train_t})\n",
    "        print ('Step: %d, Loss: %f, Accuracy: %f'\n",
    "               % (i, loss_val, acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 2000\n",
    "\n",
    "    # Fit all training data\n",
    "for epoch in range(training_epochs):   \n",
    "    \n",
    "        # Run the initializer\n",
    "        sess.run(init)            \n",
    "        sess.run(train_step, feed_dict={x:train_features, t:train_label})      \n",
    "        \n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            loss_val, acc_val = sess.run([loss, accuracy], feed_dict={x:train_features, t:train_label})\n",
    "            print ('Step: %d, Loss: %f, Accuracy: %f' % (i, loss_val, acc_val))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 4\n",
    "\n",
    "a) Use tensorflow to learn the linear regression  model y = WX + b where X is a vector of the 16 variables of\n",
    "the problem (therefore this is a linear model of 16 variables). \n",
    "\n",
    "b) Train the model using the stochastic gradient decent and the Momentum classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the yacht_hydrodynamics dataset from https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics\n",
    "\n",
    "The goal of this dataset is the prediction of residuary resistance of sailing yachts from a number of features.  Essential inputs include the basic hull dimensions and the boat velocity. \n",
    "\n",
    "This can be approached as a regression pro blem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics\n",
    "data = np.loadtxt('yacht_hydrodynamics.data')\n",
    "\n",
    "# The Pandas dataframe is created\n",
    "df = pd.DataFrame(data,columns=['Long. position', 'Prismatic coef.', 'LD ratio', 'BD ratio', 'LB ratio', 'Froude numb.', 'Resistance' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the dataset\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[:,:6]\n",
    "target = data[:,6]\n",
    "\n",
    "# We split the data into two sets, training and test\n",
    "\n",
    "train_features = features[::2,:]\n",
    "train_target = target[::2]\n",
    "train_n_samples = train_features.shape[0]\n",
    "\n",
    "test_features = features[1::2,:]\n",
    "test_target = target[1::2]\n",
    "test_n_samples = test_features.shape[0]\n",
    "\n",
    "feature_selector = SelectKBest(f_regression, k=1)\n",
    "feature_selector.fit(train_features, train_target)\n",
    "\n",
    "print(feature_selector.get_support())\n",
    "\n",
    "train_sel_feature = feature_selector.transform(train_features)\n",
    "test_sel_feature = feature_selector.transform(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 5\n",
    "\n",
    "a) Use tensorflow to learn the model y = wx + b where w is the parameter of a linear model of the  yacht_hydrodynamics dataset\n",
    "\n",
    "b) Train the model using two different stochastic gradient descent methods and the Momentum optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
