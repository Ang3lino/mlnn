{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 4. Neural Networks\n",
    "## Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing the python libraries required to solve the problems\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.special import expit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate  a dataset of points that belong to two classes and are separated by a line. \n",
    "\n",
    "Each instance of the dataset has two variables. Classes are: 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points in Class A\n",
    "xA = 20 * np.random.rand(50)\n",
    "shiftA = 20 * np.random.rand(50)\n",
    "yA = (4 + xA) / 2.0 - shiftA - 0.1\n",
    "\n",
    "# Points in Class B\n",
    "xB = 20 * np.random.rand(50)\n",
    "shiftB = 20 * np.random.rand(50)\n",
    "yB = (4 + xB) / 2.0 + shiftB + 0.1\n",
    "\n",
    "# We define our set of observations (the union of points from the two classes)\n",
    "# We concatenate the vectors\n",
    "x = np.hstack((xA, xB)).reshape(-1, 1)\n",
    "y = np.hstack((yA, yB)).reshape(-1, 1)\n",
    "x_data = np.hstack((x, y))\n",
    "\n",
    "# In the vector of target values, the first 50 instances belong to one class and the next 50 instances belong \n",
    "# to the other class\n",
    "target_class = np.vstack((np.zeros((50, 1)),np.ones((50, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function PrintDecisionFunction will be used to visualize the decision functions learned by different ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintDecisionFunction(coefs, intersect, xA, yA, xB, yB, x):\n",
    "\n",
    "     fsize = 14\n",
    "     \n",
    "    # The decision function is computed using the coefficients and intersect learned\n",
    "     # by the algorithm\n",
    "     decision_function = intersect - coefs[0] * x / coefs[1] \n",
    "        \n",
    "     fig = plt.figure()\n",
    "    \n",
    "     # The decision function is plotted\n",
    "     plt.plot(x, decision_function, 'y*', lw=4)\n",
    "        \n",
    "     # The points from the two classes are plotted\n",
    "     plt.plot(xA, yA, 'ro', lw=4)\n",
    "     plt.plot(xB, yB, 'bs', lw=4)\n",
    "\n",
    "\n",
    "     blue_patch = mpatches.Patch(color='blue', label='Class I')\n",
    "     red_patch = mpatches.Patch(color='red', label='Class II')\n",
    "     plt.legend(handles=[blue_patch,red_patch])\n",
    "     plt.xlabel(r'$x$', fontsize=fsize)\n",
    "     plt.ylabel(r'$y$', fontsize=fsize)\n",
    "\n",
    "     plt.show()\n",
    "        \n",
    "     return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "The  functions included in the following cell implement a perceptron.\n",
    "\n",
    "1) Complete functions \"Make_Predictions\" and \"Update_Weights\".\n",
    "\n",
    "\n",
    "2) Execute the subsequent cell to visualize how \"LearnPerceptron\" works.\n",
    "\n",
    "\n",
    "3) Modify the perceptron algorithm in such a way that it starts from a vector of random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb # pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 100. Number of variables: 2. Plus one variable that represents the bias.\n",
      "epoch_jt [ 0.  0.  0.] [  0.64261593 -17.24275683   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  0.69992206 -11.78362927   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  7.5431651  -12.98787181   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 0.29304406 -2.06657144  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 7.26098421 -8.41419221  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 12.14867036   0.13006574   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 3.07643993  2.58298133  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  1.52393895 -16.0539845    1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 14.3152625   -8.47944364   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 7.1489091  -4.42252154  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 13.05657782  -6.00236203   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 4.23894892 -4.46693695  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 3.82032728 -3.16728978  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 3.8414638  -7.48971973  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 19.00391394  -6.94239692   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 7.52521664 -8.63870318  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 16.83328359  -7.41326664   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 11.96361276  -6.62648984   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 0.97174879 -9.75164682  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 19.05868997   4.08800288   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 9.66959078 -3.42329411  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 5.87690846 -2.34686456  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 1.77361969 -9.96284359  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 6.04865117 -2.99116613  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 19.60085316  -8.18635024   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  2.90506447 -12.53332332   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 11.89281921   5.79226429   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 6.16936422  4.31423284  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 8.63806677  2.62795392  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 12.12252004 -10.34184137   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 11.39007383  -4.99163486   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 10.01860462   6.50798651   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  5.65089412 -11.40052354   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 19.37476807  11.55996896   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 16.4131473   -6.77411332   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 13.60325637  -0.24570935   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 17.93532772   0.71091166   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 10.07228273 -12.67136619   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 16.61513645   2.28911804   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 15.16553923  -1.63574144   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 9.63944533 -1.85798901  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 18.9160913   10.34853384   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 5.53405221 -6.45520474  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 18.77431424  -2.91406193   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 16.64237973  -2.02409889   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 12.92738505 -10.60202638   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  4.69043761 -12.54929896   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 18.81208155  -3.24906561   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 4.12824533 -7.04011027  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  0.35311423 -13.27664768   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 3.08972597  4.88635344  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 17.64634414  19.8483475    1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 10.50557586  17.04340997   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  2.9656897   19.32853707   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 12.92717379  15.68440537   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  5.1430796   18.17131207   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  2.21300025  14.11489319   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  5.03374081  21.61616135   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  4.95930305  14.80070153   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 18.85602956  21.94811482   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  4.4420294   20.27984048   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  4.10416461  19.44098121   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 1.48914714  4.01669042  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 1.12187938  4.50424625  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  2.6980773  23.1975449   1.       ]\n",
      "epoch_jt [ 0.  0.  0.] [ 15.11056142  18.41211596   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 13.59408019  21.86719406   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 18.46663518  18.28687241   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  8.22968666  22.96125015   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  9.80497315  12.73832194   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 10.62628562  18.81154337   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  2.22050487  13.85146403   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 10.80997511   9.71685069   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 0.0360931   7.47256986  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  9.61721544  17.9915518    1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 15.61232939  12.46149398   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  5.72554651  13.43992935   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  8.92892289  20.92481233   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 19.36761072  17.05087333   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  3.37426337  20.25027009   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  0.86584799  11.98435828   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 18.41946094  13.81677479   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  2.07777217  16.86745021   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 15.01772308  12.32408971   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 19.71370101  20.61576742   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  1.79849492  17.40681944   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 3.0268061   9.62808159  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  6.61195963  23.03784778   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 12.81783089  20.0248659    1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 11.46026255  21.67913674   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 11.93150764  16.18162341   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 11.81800131  14.83627074   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 18.34010528  31.17725281   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 17.58040115  19.84194754   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  8.47902207  21.12530492   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  0.58103107  20.3101739    1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [  9.06612929  24.22839766   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 10.77876393  17.64928441   1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 7.6337911   6.57294544  1.        ]\n",
      "epoch_jt [ 0.  0.  0.] [ 13.0980686  15.7901751   1.       ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,3) (100,100) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-775d1cd197d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mmy_perceptron_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_perceptron_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_perceptron_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearnPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-775d1cd197d4>\u001b[0m in \u001b[0;36mLearnPerceptron\u001b[0;34m(train_data, train_class, learning_rate, number_epochs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Using the differences the weights are updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUpdate_Weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menlarged_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_differences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m#weights = Update_Weights(weights, train_class, all_differences, learning_rate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-775d1cd197d4>\u001b[0m in \u001b[0;36mUpdate_Weights\u001b[0;34m(w, data, differences, lrate)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mUpdate_Weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdifferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdifferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,3) (100,100) "
     ]
    }
   ],
   "source": [
    "def Init_Weights(nweights):\n",
    "    weights = np.zeros(shape=(1, nweights))  \n",
    "    return weights\n",
    "\n",
    "def Make_Predictions(weights, train_data):\n",
    "    #\n",
    "    #pdb.set_trace()\n",
    "    return np.array([epoch_jt(weights[0], x) for x in train_data])\n",
    "\n",
    "\n",
    "# (list, list, list, number)\n",
    "def Update_Weights(w, data, differences, lrate): \n",
    "    #pdb.set_trace()\n",
    "    w += (data - differences)*lrate\n",
    "    return w\n",
    "\n",
    "# w: ndarray 1d, x: 1d\n",
    "def epoch_jt(w, x):\n",
    "    # pdb.set_trace()\n",
    "    # result = sum(w * x)\n",
    "    try:\n",
    "        print(\"epoch_jt\", w, x)\n",
    "        acc = sum(wi*)\n",
    "        return 1 if acc >= 0 else 0\n",
    "    except:\n",
    "        # pdb.set_trace()\n",
    "        print('FAIILL')\n",
    "        return 1\n",
    "\n",
    "\n",
    "def LearnPerceptron(train_data, train_class, learning_rate, number_epochs):\n",
    "    \n",
    "    # pdb.set_trace()\n",
    "    # Number of instances in the dataset\n",
    "    N = train_data.shape[0]  \n",
    "\n",
    "    # We enlarge the dataset adding a column of ones\n",
    "    enlarged_train_data = np.hstack((train_data,np.ones((N, 1))))\n",
    "\n",
    "    # Number of variables plus the bias \n",
    "    n = enlarged_train_data.shape[1]  \n",
    "\n",
    "    print(\"Number of instances: \"+str(N)+\". Number of variables: \"+str(n - 1)+\". Plus one variable that represents the bias.\")\n",
    "    # Weights are initialized \n",
    "    weights = Init_Weights(n)\n",
    "    error = 0\n",
    "    epoch = 0\n",
    "\n",
    "    while epoch == 0 or (error > 0 and epoch < number_epochs):\n",
    "    \n",
    "        # pdb.set_trace()\n",
    "        # The perceptron is used to make predictions  \n",
    "        predicted = Make_Predictions(weights, enlarged_train_data)\n",
    "        #pdb.set_trace()\n",
    "\n",
    "        # For each instance, we compute the difference between the prediction and the class   \n",
    "        all_differences =  predicted.T - train_class   \n",
    "\n",
    "        # Using the differences the weights are updated        \n",
    "        weights = Update_Weights(weights, enlarged_train_data, all_differences, learning_rate)       \n",
    "        #weights = Update_Weights(weights, train_class, all_differences, learning_rate)       \n",
    "\n",
    "        epoch += 1        \n",
    "\n",
    "        # We compute the error\n",
    "        error = sum(all_differences ** 2) / N\n",
    "        print(\"Epoch :\" + str(epoch) + \" Error: \" + str(error) + \" Weights: \", weights)      \n",
    "        fig = PrintDecisionFunction(weights[0, :2], weights[0, 2], xA, yA, xB, yB, x)\n",
    "\n",
    "    return error, predicted, weights\n",
    "\n",
    "\n",
    "      \n",
    "learning_rate = 0.1\n",
    "number_epochs = 15\n",
    "\n",
    "\n",
    "my_perceptron_error, my_perceptron_predictions, my_perceptron_weights = LearnPerceptron(x_data, target_class, learning_rate, number_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the following cell you can check how your implementation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "number_epochs = 15\n",
    "\n",
    "\n",
    "my_perceptron_error, my_perceptron_predictions, my_perceptron_weights = LearnPerceptron(x_data, target_class, learning_rate, number_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the prediction given by the Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(my_perceptron_predictions[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we use the scikit-learn implementation of the perceptron and learn the model using our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  Perceptron\n",
    "clf = Perceptron(max_iter=1000, tol=1e-3)\n",
    "clf.fit(x_data, target_class[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Use function \"PrintDecisionFunction\" to visualize the hyperplane learned by the Perceptron model.\n",
    "\n",
    "\n",
    "Suggestion: Take a look at the vars() function of the clf object, or the scikit-learn help for the internal parameters of class Perceptron and pass the relevant parameters to function \"PrintDecisionFunction\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_fig = PrintDecisionFunction(___, ___, xA, yA, xB, yB, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "In this exercise we will use the \"Planning Relax Data Set\" available from http://archive.ics.uci.edu/ml/datasets/Planning+Relax#\n",
    "    \n",
    "This dataset contains 12 features extracted from the analysis of EEG signals collected for 5 times on various days from a healthy right-handed subject of 25 years of age.  \n",
    "    \n",
    "The main aim of the data is to classify each instance between normal relaxed state and movement imagery.\n",
    "    \n",
    "This can be seen as a binary classification problem. \n",
    "    \n",
    "    \n",
    "3.1) Create a pipeline that:\n",
    "    \n",
    " - Imputes the data\n",
    "    \n",
    " - Standarizes the data\n",
    "    \n",
    " - Reduces the set of 12 features to only two features by dimensionality reduction.\n",
    "    \n",
    " - Applies a perceptron to classify between the two classes.\n",
    "    \n",
    "3.2) Evaluate the accuracy of the pipeline using the appropriate function of scikit-learn.\n",
    "    \n",
    "3.3) Print the confusion matrix produced by your pipeline.\n",
    "    \n",
    "3.4) Adapt the implementation of Exercise 1 so that you are able to use it in your pipeline instead of the sklearn Perceptron\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 12 columns of the file 'plrx.txt' contain the features and the last column is the class. \n",
    "\n",
    "dataset = np.loadtxt('plrx.txt')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the functions defined in Exercise 1 to change the perceptron representation. Instead of adding a column of '1's at the end of the database, treat the *theta* parameter as a separate value from the weights. \n",
    "\n",
    "4.1) Modify the values of the parameter *theta* and the learning rate to observe the effect on the final solution.\n",
    "\n",
    "4.2) Imagine a method that is able to reduce the *size of the steps* given in each iteration of the learning algorithm.\n",
    "        \n",
    "        Tip: The size of the steps is regulated by the learning rate multiplier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Init_Weights(nweights):\n",
    "    weights = np.zeros(shape=(1, nweights))  \n",
    "    return weights\n",
    "\n",
    "\n",
    "def Make_Predictions(weights, theta, train_data):\n",
    "    return preds\n",
    "\n",
    "\n",
    "\n",
    "def Update_Weights(w, theta, data, differences, lrate): \n",
    "    return w, theta\n",
    " \n",
    "    \n",
    "\n",
    "def LearnPerceptron(train_data, train_class, learning_rate, number_epochs):\n",
    "\n",
    "   # Number of instances in the dataset\n",
    "   N = train_data.shape[0]   \n",
    "\n",
    "   weights = Init_Weights(train_data.shape[1])\n",
    "   theta = 0\n",
    "   error = 0\n",
    "   epoch = 0\n",
    "\n",
    "   while epoch == 0 or (error > 0 and epoch < number_epochs):\n",
    "        \n",
    "      # The perceptron is used to make predictions  \n",
    "      predicted = Make_Predictions(weights, theta, train_data)\n",
    "             \n",
    "      # For each instance, we compute the difference between the prediction and the class   \n",
    "      all_differences = train_class - predicted      \n",
    "      \n",
    "      # Using the differences the weights are updated        \n",
    "      weights, theta = Update_Weights(weights, theta, train_data, all_differences, learning_rate)       \n",
    "      \n",
    "      epoch = epoch + 1        \n",
    "      \n",
    "      # We compute the error\n",
    "      error = sum(all_differences ** 2) / N\n",
    "      print(\"Epoch :\" + str(epoch) + \" Error: \" + str(error) + \" Weights: \", weights, \"Theta:\", theta)      \n",
    "      fig = PrintDecisionFunction(weights[0, :2], theta, xA, yA, xB, yB, x)\n",
    "    \n",
    "   return error, predicted, weights\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "number_epochs = 15\n",
    "\n",
    "\n",
    "my_perceptron_error,my_perceptron_predictions,my_perceptron_weights= LearnPerceptron(x_data, target_class, learning_rate, number_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
