{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic 4. Neural Networks\n",
    "## Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing the python libraries required to solve the problems\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.special import expit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate  a dataset of points that belong to two classes and are separated by a line. \n",
    "\n",
    "Each instance of the dataset has two variables. Classes are: 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points in Class A\n",
    "xA = 20 * np.random.rand(50)\n",
    "shiftA = 20 * np.random.rand(50)\n",
    "yA = (4 + xA) / 2.0 - shiftA - 0.1\n",
    "\n",
    "# Points in Class B\n",
    "xB = 20 * np.random.rand(50)\n",
    "shiftB = 20 * np.random.rand(50)\n",
    "yB = (4 + xB) / 2.0 + shiftB + 0.1\n",
    "\n",
    "# We define our set of observations (the union of points from the two classes)\n",
    "# We concatenate the vectors\n",
    "x = np.hstack((xA, xB)).reshape(-1, 1)\n",
    "y = np.hstack((yA, yB)).reshape(-1, 1)\n",
    "x_data = np.hstack((x, y))\n",
    "\n",
    "# In the vector of target values, the first 50 instances belong to one class and the next 50 instances belong \n",
    "# to the other class\n",
    "target_class = np.vstack((np.zeros((50, 1)),np.ones((50, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function PrintDecisionFunction will be used to visualize the decision functions learned by different ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintDecisionFunction(coefs, intersect, xA, yA, xB, yB, x):\n",
    "\n",
    "     fsize = 14\n",
    "     \n",
    "    # The decision function is computed using the coefficients and intersect learned\n",
    "     # by the algorithm\n",
    "     decision_function = intersect - coefs[0] * x / coefs[1] \n",
    "        \n",
    "     fig = plt.figure()\n",
    "    \n",
    "     # The decision function is plotted\n",
    "     plt.plot(x, decision_function, 'y*', lw=4)\n",
    "        \n",
    "     # The points from the two classes are plotted\n",
    "     plt.plot(xA, yA, 'ro', lw=4)\n",
    "     plt.plot(xB, yB, 'bs', lw=4)\n",
    "\n",
    "\n",
    "     blue_patch = mpatches.Patch(color='blue', label='Class I')\n",
    "     red_patch = mpatches.Patch(color='red', label='Class II')\n",
    "     plt.legend(handles=[blue_patch,red_patch])\n",
    "     plt.xlabel(r'$x$', fontsize=fsize)\n",
    "     plt.ylabel(r'$y$', fontsize=fsize)\n",
    "\n",
    "     plt.show()\n",
    "        \n",
    "     return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "The  functions included in the following cell implement a perceptron.\n",
    "\n",
    "1) Complete functions \"Make_Predictions\" and \"Update_Weights\".\n",
    "\n",
    "\n",
    "2) Execute the subsequent cell to visualize how \"LearnPerceptron\" works.\n",
    "\n",
    "\n",
    "3) Modify the perceptron algorithm in such a way that it starts from a vector of random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 100. Number of variables: 2. Plus one variable that represents the bias.\n",
      "> <ipython-input-6-1d5c70bbc28d>(7)make_pred()\n",
      "-> return heaviside(np.dot(weights[0], xi))\n",
      "(Pdb) c\n",
      "> <ipython-input-6-1d5c70bbc28d>(7)make_pred()\n",
      "-> return heaviside(np.dot(weights[0], xi))\n"
     ]
    }
   ],
   "source": [
    "def Init_Weights(nweights):\n",
    "    weights = np.zeros(shape=(1, nweights))  \n",
    "    return weights\n",
    "\n",
    "def make_pred(weights, xi):\n",
    "    set_trace()\n",
    "    return heaviside(np.dot(weights[0], xi))\n",
    "\n",
    "def Make_Predictions(weights, train_data):\n",
    "    return np.array([make_pred(weights, x) for x in train_data])\n",
    "\n",
    "def heaviside(z):\n",
    "\n",
    "    return 1 if z >= 0 else 0\n",
    "\n",
    "# (list, list, list, number)\n",
    "def Update_Weights(w, X, differences, lrate): \n",
    "    for i, _ in enumerate(differences):\n",
    "        for j, __ in enumerate(w):\n",
    "            w[j] += differences[j] * X[i][j] * lrate\n",
    "    return w\n",
    "\n",
    "def LearnPerceptron(train_data, train_class, learning_rate, number_epochs):\n",
    "    \n",
    "    # pdb.set_trace()\n",
    "    # Number of instances in the dataset\n",
    "    N = train_data.shape[0]  \n",
    "\n",
    "    # We enlarge the dataset adding a column of ones\n",
    "    enlarged_train_data = np.hstack((train_data,np.ones((N, 1))))\n",
    "\n",
    "    # Number of variables plus the bias \n",
    "    n = enlarged_train_data.shape[1]  \n",
    "\n",
    "    print(\"Number of instances: \"+str(N)+\". Number of variables: \"+str(n - 1)+\". Plus one variable that represents the bias.\")\n",
    "    # Weights are initialized \n",
    "    weights = Init_Weights(n)\n",
    "    error = 0\n",
    "    epoch = 0\n",
    "\n",
    "    while epoch == 0 or (error > 0 and epoch < number_epochs):\n",
    "    \n",
    "        # pdb.set_trace()\n",
    "        # The perceptron is used to make predictions  \n",
    "        predicted = Make_Predictions(weights, enlarged_train_data)\n",
    "        #pdb.set_trace()\n",
    "\n",
    "        # For each instance, we compute the difference between the prediction and the class   \n",
    "        all_differences =  predicted.T - train_class   \n",
    "\n",
    "        # Using the differences the weights are updated        \n",
    "        weights = Update_Weights(weights, enlarged_train_data, all_differences, learning_rate)       \n",
    "        #weights = Update_Weights(weights, train_class, all_differences, learning_rate)       \n",
    "\n",
    "        epoch += 1        \n",
    "\n",
    "        # We compute the error\n",
    "        error = sum(all_differences ** 2) / N\n",
    "        print(\"Epoch :\" + str(epoch) + \" Error: \" + str(error) + \" Weights: \", weights)      \n",
    "        fig = PrintDecisionFunction(weights[0, :2], weights[0, 2], xA, yA, xB, yB, x)\n",
    "\n",
    "    return error, predicted, weights\n",
    "\n",
    "\n",
    "      \n",
    "learning_rate = 0.1\n",
    "number_epochs = 15\n",
    "\n",
    "\n",
    "my_perceptron_error, my_perceptron_predictions, my_perceptron_weights = LearnPerceptron(x_data, target_class, learning_rate, number_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the following cell you can check how your implementation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "number_epochs = 15\n",
    "\n",
    "\n",
    "my_perceptron_error, my_perceptron_predictions, my_perceptron_weights = LearnPerceptron(x_data, target_class, learning_rate, number_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the prediction given by the Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(my_perceptron_predictions[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we use the scikit-learn implementation of the perceptron and learn the model using our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  Perceptron\n",
    "clf = Perceptron(max_iter=1000, tol=1e-3)\n",
    "clf.fit(x_data, target_class[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Use function \"PrintDecisionFunction\" to visualize the hyperplane learned by the Perceptron model.\n",
    "\n",
    "\n",
    "Suggestion: Take a look at the vars() function of the clf object, or the scikit-learn help for the internal parameters of class Perceptron and pass the relevant parameters to function \"PrintDecisionFunction\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_fig = PrintDecisionFunction(___, ___, xA, yA, xB, yB, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "In this exercise we will use the \"Planning Relax Data Set\" available from http://archive.ics.uci.edu/ml/datasets/Planning+Relax#\n",
    "    \n",
    "This dataset contains 12 features extracted from the analysis of EEG signals collected for 5 times on various days from a healthy right-handed subject of 25 years of age.  \n",
    "    \n",
    "The main aim of the data is to classify each instance between normal relaxed state and movement imagery.\n",
    "    \n",
    "This can be seen as a binary classification problem. \n",
    "    \n",
    "    \n",
    "3.1) Create a pipeline that:\n",
    "    \n",
    " - Imputes the data\n",
    "    \n",
    " - Standarizes the data\n",
    "    \n",
    " - Reduces the set of 12 features to only two features by dimensionality reduction.\n",
    "    \n",
    " - Applies a perceptron to classify between the two classes.\n",
    "    \n",
    "3.2) Evaluate the accuracy of the pipeline using the appropriate function of scikit-learn.\n",
    "    \n",
    "3.3) Print the confusion matrix produced by your pipeline.\n",
    "    \n",
    "3.4) Adapt the implementation of Exercise 1 so that you are able to use it in your pipeline instead of the sklearn Perceptron\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 12 columns of the file 'plrx.txt' contain the features and the last column is the class. \n",
    "\n",
    "dataset = np.loadtxt('plrx.txt')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the functions defined in Exercise 1 to change the perceptron representation. Instead of adding a column of '1's at the end of the database, treat the *theta* parameter as a separate value from the weights. \n",
    "\n",
    "4.1) Modify the values of the parameter *theta* and the learning rate to observe the effect on the final solution.\n",
    "\n",
    "4.2) Imagine a method that is able to reduce the *size of the steps* given in each iteration of the learning algorithm.\n",
    "        \n",
    "        Tip: The size of the steps is regulated by the learning rate multiplier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Init_Weights(nweights):\n",
    "    weights = np.zeros(shape=(1, nweights))  \n",
    "    return weights\n",
    "\n",
    "\n",
    "def Make_Predictions(weights, theta, train_data):\n",
    "    return preds\n",
    "\n",
    "\n",
    "\n",
    "def Update_Weights(w, theta, data, differences, lrate): \n",
    "    return w, theta\n",
    " \n",
    "    \n",
    "\n",
    "def LearnPerceptron(train_data, train_class, learning_rate, number_epochs):\n",
    "\n",
    "   # Number of instances in the dataset\n",
    "   N = train_data.shape[0]   \n",
    "\n",
    "   weights = Init_Weights(train_data.shape[1])\n",
    "   theta = 0\n",
    "   error = 0\n",
    "   epoch = 0\n",
    "\n",
    "   while epoch == 0 or (error > 0 and epoch < number_epochs):\n",
    "        \n",
    "      # The perceptron is used to make predictions  \n",
    "      predicted = Make_Predictions(weights, theta, train_data)\n",
    "             \n",
    "      # For each instance, we compute the difference between the prediction and the class   \n",
    "      all_differences = train_class - predicted      \n",
    "      \n",
    "      # Using the differences the weights are updated        \n",
    "      weights, theta = Update_Weights(weights, theta, train_data, all_differences, learning_rate)       \n",
    "      \n",
    "      epoch = epoch + 1        \n",
    "      \n",
    "      # We compute the error\n",
    "      error = sum(all_differences ** 2) / N\n",
    "      print(\"Epoch :\" + str(epoch) + \" Error: \" + str(error) + \" Weights: \", weights, \"Theta:\", theta)      \n",
    "      fig = PrintDecisionFunction(weights[0, :2], theta, xA, yA, xB, yB, x)\n",
    "    \n",
    "   return error, predicted, weights\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "number_epochs = 15\n",
    "\n",
    "\n",
    "my_perceptron_error,my_perceptron_predictions,my_perceptron_weights= LearnPerceptron(x_data, target_class, learning_rate, number_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
